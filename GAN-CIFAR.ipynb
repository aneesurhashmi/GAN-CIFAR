{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR4NOUbjAAWQ"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "import cv2 "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE4LviT8AEa8"
      },
      "source": [
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nShOIOHaAM1f"
      },
      "source": [
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\tn_nodes = 256 * 4 * 4\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((4, 4, 256)))\n",
        "\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\treturn model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_xO-FyxAO6K"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\td_model.trainable = False\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\tmodel.add(g_model)\n",
        "\n",
        "\tmodel.add(d_model)\n",
        "\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Wmdxp9AQyi"
      },
      "source": [
        "def load_real_samples():\n",
        "\t(trainX, _), (_, _) = load_data()\n",
        "\tX = trainX.astype('float32')\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X\n",
        " \n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\tX = dataset[ix]\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJXwiySATfY"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        " \n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\tX = g_model.predict(x_input)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gVYAUD8AV71"
      },
      "source": [
        "def save_plot(examples, epoch, n=7):\n",
        "\texamples = (examples + 1) / 2.0\n",
        "\tfor i in range(n * n):\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(examples[i])\n",
        "\tfilename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(filename)\n",
        "\tpyplot.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLgBsNy1AZz5"
      },
      "source": [
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\tsave_plot(x_fake, epoch)\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(filename)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FglaJuMKAb4r"
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\tfor i in range(n_epochs):\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\td_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\td_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\t\tprint('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "\t\t\t\t(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtlBA5E8Ag_Z",
        "outputId": "38cf3fde-5f11-4377-dfd5-37123419e990"
      },
      "source": [
        "latent_dim = 100\n",
        "d_model = define_discriminator()\n",
        "g_model = define_generator(latent_dim)\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "dataset = load_real_samples()\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n",
            ">1, 1/390, d1=0.694, d2=0.696 g=0.691\n",
            ">1, 2/390, d1=0.634, d2=0.700 g=0.687\n",
            ">1, 3/390, d1=0.570, d2=0.708 g=0.680\n",
            ">1, 4/390, d1=0.495, d2=0.728 g=0.662\n",
            ">1, 5/390, d1=0.401, d2=0.772 g=0.632\n",
            ">1, 6/390, d1=0.311, d2=0.841 g=0.602\n",
            ">1, 7/390, d1=0.239, d2=0.887 g=0.606\n",
            ">1, 8/390, d1=0.187, d2=0.833 g=0.681\n",
            ">1, 9/390, d1=0.141, d2=0.681 g=0.821\n",
            ">1, 10/390, d1=0.109, d2=0.583 g=0.946\n",
            ">1, 11/390, d1=0.066, d2=0.555 g=0.981\n",
            ">1, 12/390, d1=0.062, d2=0.594 g=0.933\n",
            ">1, 13/390, d1=0.060, d2=0.682 g=0.868\n",
            ">1, 14/390, d1=0.066, d2=0.761 g=0.810\n",
            ">1, 15/390, d1=0.150, d2=0.865 g=0.728\n",
            ">1, 16/390, d1=0.138, d2=0.965 g=0.707\n",
            ">1, 17/390, d1=0.148, d2=0.878 g=0.787\n",
            ">1, 18/390, d1=0.207, d2=0.693 g=0.918\n",
            ">1, 19/390, d1=0.369, d2=0.581 g=0.957\n",
            ">1, 20/390, d1=0.139, d2=0.528 g=0.996\n",
            ">1, 21/390, d1=0.183, d2=0.524 g=0.975\n",
            ">1, 22/390, d1=0.087, d2=0.544 g=0.926\n",
            ">1, 23/390, d1=0.057, d2=0.570 g=0.880\n",
            ">1, 24/390, d1=0.041, d2=0.605 g=0.848\n",
            ">1, 25/390, d1=0.029, d2=0.668 g=0.820\n",
            ">1, 26/390, d1=0.023, d2=0.844 g=0.777\n",
            ">1, 27/390, d1=0.043, d2=0.940 g=0.894\n",
            ">1, 28/390, d1=0.118, d2=0.646 g=1.297\n",
            ">1, 29/390, d1=0.358, d2=0.502 g=1.075\n",
            ">1, 30/390, d1=0.064, d2=0.488 g=1.185\n",
            ">1, 31/390, d1=0.045, d2=0.407 g=1.241\n",
            ">1, 32/390, d1=0.036, d2=0.436 g=1.130\n",
            ">1, 33/390, d1=0.045, d2=0.491 g=1.005\n",
            ">1, 34/390, d1=0.015, d2=0.520 g=0.978\n",
            ">1, 35/390, d1=0.011, d2=0.573 g=0.961\n",
            ">1, 36/390, d1=0.026, d2=0.678 g=0.973\n",
            ">1, 37/390, d1=0.040, d2=0.653 g=1.245\n",
            ">1, 38/390, d1=0.076, d2=0.377 g=1.801\n",
            ">1, 39/390, d1=0.379, d2=0.384 g=1.495\n",
            ">1, 40/390, d1=0.034, d2=0.330 g=2.068\n",
            ">1, 41/390, d1=0.087, d2=0.166 g=2.260\n",
            ">1, 42/390, d1=0.089, d2=0.204 g=1.992\n",
            ">1, 43/390, d1=0.045, d2=0.242 g=2.128\n",
            ">1, 44/390, d1=0.111, d2=0.211 g=2.239\n",
            ">1, 45/390, d1=0.076, d2=0.233 g=2.609\n",
            ">1, 46/390, d1=0.096, d2=0.114 g=2.815\n",
            ">1, 47/390, d1=0.046, d2=0.117 g=2.815\n",
            ">1, 48/390, d1=0.019, d2=0.135 g=2.805\n",
            ">1, 49/390, d1=0.022, d2=0.157 g=2.942\n",
            ">1, 50/390, d1=0.068, d2=0.132 g=2.746\n",
            ">1, 51/390, d1=0.010, d2=0.163 g=2.650\n",
            ">1, 52/390, d1=0.105, d2=0.475 g=2.982\n",
            ">1, 53/390, d1=0.106, d2=0.117 g=3.054\n",
            ">1, 54/390, d1=0.334, d2=0.331 g=2.212\n",
            ">1, 55/390, d1=0.042, d2=0.422 g=4.056\n",
            ">1, 56/390, d1=0.109, d2=0.017 g=4.401\n",
            ">1, 57/390, d1=0.115, d2=0.036 g=3.404\n",
            ">1, 58/390, d1=0.020, d2=0.091 g=2.929\n",
            ">1, 59/390, d1=0.031, d2=0.230 g=3.237\n",
            ">1, 60/390, d1=0.057, d2=0.435 g=4.600\n",
            ">1, 61/390, d1=0.161, d2=0.062 g=4.328\n",
            ">1, 62/390, d1=0.131, d2=2.447 g=7.971\n",
            ">1, 63/390, d1=2.729, d2=0.196 g=2.073\n",
            ">1, 64/390, d1=0.054, d2=2.139 g=3.441\n",
            ">1, 65/390, d1=0.469, d2=0.102 g=3.980\n",
            ">1, 66/390, d1=0.546, d2=0.224 g=3.257\n",
            ">1, 67/390, d1=0.783, d2=0.605 g=4.768\n",
            ">1, 68/390, d1=0.407, d2=0.008 g=5.428\n",
            ">1, 69/390, d1=0.445, d2=0.033 g=3.586\n",
            ">1, 70/390, d1=0.176, d2=0.298 g=3.801\n",
            ">1, 71/390, d1=0.249, d2=0.191 g=4.465\n",
            ">1, 72/390, d1=0.275, d2=0.065 g=4.330\n",
            ">1, 73/390, d1=0.245, d2=0.255 g=6.316\n",
            ">1, 74/390, d1=0.333, d2=0.008 g=6.342\n",
            ">1, 75/390, d1=0.172, d2=0.756 g=8.822\n",
            ">1, 76/390, d1=0.441, d2=0.009 g=7.109\n",
            ">1, 77/390, d1=0.677, d2=2.602 g=9.323\n",
            ">1, 78/390, d1=2.866, d2=0.017 g=4.234\n",
            ">1, 79/390, d1=0.924, d2=0.294 g=1.508\n",
            ">1, 80/390, d1=0.601, d2=0.674 g=1.291\n",
            ">1, 81/390, d1=0.506, d2=0.621 g=1.581\n",
            ">1, 82/390, d1=0.437, d2=0.313 g=1.967\n",
            ">1, 83/390, d1=0.644, d2=0.333 g=1.746\n",
            ">1, 84/390, d1=0.580, d2=0.355 g=1.682\n",
            ">1, 85/390, d1=0.440, d2=0.362 g=1.613\n",
            ">1, 86/390, d1=0.326, d2=0.353 g=1.611\n",
            ">1, 87/390, d1=0.387, d2=0.386 g=1.590\n",
            ">1, 88/390, d1=0.228, d2=0.299 g=1.637\n",
            ">1, 89/390, d1=0.230, d2=0.278 g=1.748\n",
            ">1, 90/390, d1=0.158, d2=0.249 g=1.802\n",
            ">1, 91/390, d1=0.193, d2=0.217 g=1.769\n",
            ">1, 92/390, d1=0.097, d2=0.254 g=1.705\n",
            ">1, 93/390, d1=0.107, d2=0.263 g=1.732\n",
            ">1, 94/390, d1=0.071, d2=0.282 g=1.587\n",
            ">1, 95/390, d1=0.087, d2=0.478 g=1.428\n",
            ">1, 96/390, d1=0.182, d2=0.816 g=1.330\n",
            ">1, 97/390, d1=0.120, d2=1.267 g=1.226\n",
            ">1, 98/390, d1=0.238, d2=0.661 g=1.389\n",
            ">1, 99/390, d1=0.305, d2=0.433 g=1.608\n",
            ">1, 100/390, d1=0.305, d2=0.495 g=2.106\n",
            ">1, 101/390, d1=0.498, d2=0.247 g=2.564\n",
            ">1, 102/390, d1=0.571, d2=0.148 g=2.249\n",
            ">1, 103/390, d1=0.483, d2=0.259 g=2.241\n",
            ">1, 104/390, d1=0.315, d2=0.195 g=2.243\n",
            ">1, 105/390, d1=0.576, d2=0.236 g=2.045\n",
            ">1, 106/390, d1=0.501, d2=0.336 g=2.236\n",
            ">1, 107/390, d1=0.713, d2=0.288 g=2.112\n",
            ">1, 108/390, d1=0.541, d2=0.353 g=1.983\n",
            ">1, 109/390, d1=0.816, d2=0.346 g=1.696\n",
            ">1, 110/390, d1=0.586, d2=0.369 g=1.763\n",
            ">1, 111/390, d1=0.411, d2=0.301 g=1.879\n",
            ">1, 112/390, d1=0.381, d2=0.276 g=2.108\n",
            ">1, 113/390, d1=0.538, d2=0.281 g=1.974\n",
            ">1, 114/390, d1=0.456, d2=0.303 g=2.187\n",
            ">1, 115/390, d1=0.358, d2=0.184 g=2.442\n",
            ">1, 116/390, d1=0.515, d2=0.212 g=2.153\n",
            ">1, 117/390, d1=0.574, d2=0.327 g=2.046\n",
            ">1, 118/390, d1=0.261, d2=0.170 g=2.498\n",
            ">1, 119/390, d1=0.374, d2=0.145 g=2.443\n",
            ">1, 120/390, d1=0.313, d2=0.167 g=2.435\n",
            ">1, 121/390, d1=0.253, d2=0.118 g=2.561\n",
            ">1, 122/390, d1=0.482, d2=0.199 g=2.176\n",
            ">1, 123/390, d1=0.323, d2=0.187 g=2.180\n",
            ">1, 124/390, d1=0.163, d2=0.140 g=2.383\n",
            ">1, 125/390, d1=0.150, d2=0.118 g=2.547\n",
            ">1, 126/390, d1=0.230, d2=0.120 g=2.421\n",
            ">1, 127/390, d1=0.186, d2=0.144 g=2.343\n",
            ">1, 128/390, d1=0.159, d2=0.166 g=2.205\n",
            ">1, 129/390, d1=0.238, d2=0.289 g=1.905\n",
            ">1, 130/390, d1=0.168, d2=0.556 g=1.679\n",
            ">1, 131/390, d1=0.272, d2=0.832 g=1.485\n",
            ">1, 132/390, d1=0.216, d2=0.993 g=1.241\n",
            ">1, 133/390, d1=0.226, d2=1.195 g=1.769\n",
            ">1, 134/390, d1=0.317, d2=0.261 g=2.463\n",
            ">1, 135/390, d1=0.394, d2=0.199 g=2.680\n",
            ">1, 136/390, d1=0.242, d2=0.181 g=2.638\n",
            ">1, 137/390, d1=0.502, d2=0.214 g=2.773\n",
            ">1, 138/390, d1=0.291, d2=0.166 g=2.715\n",
            ">1, 139/390, d1=0.353, d2=0.165 g=2.360\n",
            ">1, 140/390, d1=0.256, d2=0.221 g=2.477\n",
            ">1, 141/390, d1=0.339, d2=0.197 g=2.320\n",
            ">1, 142/390, d1=0.373, d2=0.324 g=2.245\n",
            ">1, 143/390, d1=0.202, d2=0.284 g=2.278\n",
            ">1, 144/390, d1=0.260, d2=0.386 g=2.326\n",
            ">1, 145/390, d1=0.444, d2=0.328 g=2.234\n",
            ">1, 146/390, d1=0.278, d2=0.379 g=2.362\n",
            ">1, 147/390, d1=0.582, d2=0.336 g=2.163\n",
            ">1, 148/390, d1=0.519, d2=0.396 g=2.352\n",
            ">1, 149/390, d1=0.533, d2=0.281 g=2.322\n",
            ">1, 150/390, d1=0.323, d2=0.213 g=2.206\n",
            ">1, 151/390, d1=0.389, d2=0.342 g=1.975\n",
            ">1, 152/390, d1=0.405, d2=0.432 g=2.013\n",
            ">1, 153/390, d1=0.170, d2=0.309 g=2.480\n",
            ">1, 154/390, d1=0.177, d2=0.161 g=2.611\n",
            ">1, 155/390, d1=0.418, d2=0.399 g=2.505\n",
            ">1, 156/390, d1=0.115, d2=0.509 g=2.783\n",
            ">1, 157/390, d1=0.250, d2=0.690 g=3.378\n",
            ">1, 158/390, d1=0.342, d2=0.229 g=3.553\n",
            ">1, 159/390, d1=0.513, d2=0.288 g=2.724\n",
            ">1, 160/390, d1=0.400, d2=0.997 g=3.466\n",
            ">1, 161/390, d1=1.022, d2=0.366 g=2.183\n",
            ">1, 162/390, d1=0.866, d2=0.695 g=1.667\n",
            ">1, 163/390, d1=0.433, d2=0.462 g=1.968\n",
            ">1, 164/390, d1=0.554, d2=0.292 g=1.895\n",
            ">1, 165/390, d1=0.612, d2=0.356 g=1.883\n",
            ">1, 166/390, d1=0.372, d2=0.257 g=1.925\n",
            ">1, 167/390, d1=0.395, d2=0.308 g=2.242\n",
            ">1, 168/390, d1=0.271, d2=0.133 g=2.573\n",
            ">1, 169/390, d1=0.396, d2=0.141 g=2.871\n",
            ">1, 170/390, d1=0.219, d2=0.130 g=3.254\n",
            ">1, 171/390, d1=0.290, d2=0.084 g=3.453\n",
            ">1, 172/390, d1=0.154, d2=0.148 g=3.650\n",
            ">1, 173/390, d1=0.205, d2=0.226 g=3.899\n",
            ">1, 174/390, d1=0.252, d2=0.069 g=4.176\n",
            ">1, 175/390, d1=0.142, d2=0.213 g=5.175\n",
            ">1, 176/390, d1=0.315, d2=0.153 g=5.060\n",
            ">1, 177/390, d1=0.335, d2=0.298 g=4.354\n",
            ">1, 178/390, d1=0.389, d2=0.396 g=4.626\n",
            ">1, 179/390, d1=0.271, d2=0.251 g=4.638\n",
            ">1, 180/390, d1=0.465, d2=2.048 g=8.283\n",
            ">1, 181/390, d1=1.922, d2=0.085 g=6.094\n",
            ">1, 182/390, d1=1.562, d2=0.142 g=2.657\n",
            ">1, 183/390, d1=0.885, d2=0.608 g=3.502\n",
            ">1, 184/390, d1=0.926, d2=0.211 g=3.884\n",
            ">1, 185/390, d1=1.033, d2=0.173 g=2.921\n",
            ">1, 186/390, d1=0.752, d2=0.265 g=2.735\n",
            ">1, 187/390, d1=0.642, d2=0.195 g=2.846\n",
            ">1, 188/390, d1=0.604, d2=0.155 g=2.903\n",
            ">1, 189/390, d1=0.692, d2=0.181 g=2.602\n",
            ">1, 190/390, d1=0.499, d2=0.182 g=2.833\n",
            ">1, 191/390, d1=0.663, d2=0.179 g=2.723\n",
            ">1, 192/390, d1=0.526, d2=0.200 g=2.847\n",
            ">1, 193/390, d1=0.583, d2=0.153 g=2.599\n",
            ">1, 194/390, d1=0.500, d2=0.251 g=2.653\n",
            ">1, 195/390, d1=0.559, d2=0.223 g=2.453\n",
            ">1, 196/390, d1=0.546, d2=0.235 g=2.165\n",
            ">1, 197/390, d1=0.465, d2=0.356 g=2.359\n",
            ">1, 198/390, d1=0.533, d2=0.365 g=2.106\n",
            ">1, 199/390, d1=0.515, d2=0.495 g=1.934\n",
            ">1, 200/390, d1=0.793, d2=0.614 g=1.764\n",
            ">1, 201/390, d1=0.849, d2=0.565 g=1.566\n",
            ">1, 202/390, d1=0.782, d2=0.510 g=1.453\n",
            ">1, 203/390, d1=0.875, d2=0.508 g=1.248\n",
            ">1, 204/390, d1=0.705, d2=0.433 g=1.301\n",
            ">1, 205/390, d1=0.570, d2=0.478 g=1.220\n",
            ">1, 206/390, d1=0.650, d2=0.492 g=1.184\n",
            ">1, 207/390, d1=0.517, d2=0.431 g=1.254\n",
            ">1, 208/390, d1=0.372, d2=0.437 g=1.334\n",
            ">1, 209/390, d1=0.452, d2=0.355 g=1.396\n",
            ">1, 210/390, d1=0.659, d2=0.450 g=1.323\n",
            ">1, 211/390, d1=0.322, d2=0.440 g=1.351\n",
            ">1, 212/390, d1=0.404, d2=0.417 g=1.390\n",
            ">1, 213/390, d1=0.475, d2=0.419 g=1.263\n",
            ">1, 214/390, d1=0.533, d2=0.512 g=1.304\n",
            ">1, 215/390, d1=0.399, d2=0.542 g=1.321\n",
            ">1, 216/390, d1=0.335, d2=0.443 g=1.350\n",
            ">1, 217/390, d1=0.254, d2=0.382 g=1.376\n",
            ">1, 218/390, d1=0.344, d2=0.550 g=1.516\n",
            ">1, 219/390, d1=0.269, d2=0.431 g=1.480\n",
            ">1, 220/390, d1=0.301, d2=0.492 g=1.488\n",
            ">1, 221/390, d1=0.305, d2=0.421 g=1.355\n",
            ">1, 222/390, d1=0.319, d2=0.410 g=1.392\n",
            ">1, 223/390, d1=0.235, d2=0.340 g=1.532\n",
            ">1, 224/390, d1=0.266, d2=0.338 g=1.592\n",
            ">1, 225/390, d1=0.252, d2=0.323 g=1.565\n",
            ">1, 226/390, d1=0.243, d2=0.361 g=1.527\n",
            ">1, 227/390, d1=0.238, d2=0.382 g=1.521\n",
            ">1, 228/390, d1=0.194, d2=0.391 g=1.464\n",
            ">1, 229/390, d1=0.366, d2=0.488 g=1.341\n",
            ">1, 230/390, d1=0.349, d2=0.482 g=1.372\n",
            ">1, 231/390, d1=0.243, d2=0.477 g=1.422\n",
            ">1, 232/390, d1=0.283, d2=0.445 g=1.392\n",
            ">1, 233/390, d1=0.286, d2=0.444 g=1.426\n",
            ">1, 234/390, d1=0.356, d2=0.448 g=1.416\n",
            ">1, 235/390, d1=0.565, d2=0.505 g=1.383\n",
            ">1, 236/390, d1=0.219, d2=0.462 g=1.476\n",
            ">1, 237/390, d1=0.311, d2=0.371 g=1.440\n",
            ">1, 238/390, d1=0.375, d2=0.476 g=1.323\n",
            ">1, 239/390, d1=0.411, d2=0.491 g=1.313\n",
            ">1, 240/390, d1=0.360, d2=0.545 g=1.324\n",
            ">1, 241/390, d1=0.338, d2=0.525 g=1.495\n",
            ">1, 242/390, d1=0.487, d2=0.560 g=1.623\n",
            ">1, 243/390, d1=0.688, d2=0.625 g=1.718\n",
            ">1, 244/390, d1=0.732, d2=0.658 g=1.643\n",
            ">1, 245/390, d1=0.862, d2=0.627 g=1.558\n",
            ">1, 246/390, d1=0.847, d2=0.525 g=1.603\n",
            ">1, 247/390, d1=0.685, d2=0.431 g=1.713\n",
            ">1, 248/390, d1=0.695, d2=0.516 g=1.660\n",
            ">1, 249/390, d1=0.600, d2=0.503 g=1.636\n",
            ">1, 250/390, d1=0.624, d2=0.385 g=1.720\n",
            ">1, 251/390, d1=0.679, d2=0.420 g=1.621\n",
            ">1, 252/390, d1=0.571, d2=0.442 g=1.615\n",
            ">1, 253/390, d1=0.463, d2=0.443 g=1.888\n",
            ">1, 254/390, d1=0.610, d2=0.459 g=1.646\n",
            ">1, 255/390, d1=0.581, d2=0.493 g=1.511\n",
            ">1, 256/390, d1=0.584, d2=0.502 g=1.746\n",
            ">1, 257/390, d1=0.701, d2=0.408 g=1.683\n",
            ">1, 258/390, d1=0.692, d2=0.466 g=1.633\n",
            ">1, 259/390, d1=0.610, d2=0.484 g=1.543\n",
            ">1, 260/390, d1=0.599, d2=0.540 g=1.843\n",
            ">1, 261/390, d1=0.947, d2=0.535 g=1.772\n",
            ">1, 262/390, d1=0.740, d2=0.419 g=1.832\n",
            ">1, 263/390, d1=0.663, d2=0.686 g=1.969\n",
            ">1, 264/390, d1=0.779, d2=0.531 g=1.734\n",
            ">1, 265/390, d1=0.627, d2=0.461 g=2.185\n",
            ">1, 266/390, d1=0.798, d2=0.488 g=1.744\n",
            ">1, 267/390, d1=0.693, d2=0.565 g=2.377\n",
            ">1, 268/390, d1=0.839, d2=0.248 g=2.089\n",
            ">1, 269/390, d1=0.757, d2=0.486 g=1.667\n",
            ">1, 270/390, d1=0.529, d2=0.446 g=1.948\n",
            ">1, 271/390, d1=0.684, d2=0.353 g=1.857\n",
            ">1, 272/390, d1=0.668, d2=0.467 g=1.756\n",
            ">1, 273/390, d1=0.556, d2=0.502 g=1.602\n",
            ">1, 274/390, d1=0.427, d2=0.389 g=1.970\n",
            ">1, 275/390, d1=0.735, d2=0.433 g=1.544\n",
            ">1, 276/390, d1=0.398, d2=0.436 g=1.558\n",
            ">1, 277/390, d1=0.499, d2=0.468 g=1.545\n",
            ">1, 278/390, d1=0.546, d2=0.496 g=1.455\n",
            ">1, 279/390, d1=0.492, d2=0.453 g=1.579\n",
            ">1, 280/390, d1=0.450, d2=0.386 g=1.795\n",
            ">1, 281/390, d1=0.405, d2=0.335 g=1.793\n",
            ">1, 282/390, d1=0.554, d2=0.386 g=1.578\n",
            ">1, 283/390, d1=0.481, d2=0.561 g=1.450\n",
            ">1, 284/390, d1=0.525, d2=0.675 g=1.476\n",
            ">1, 285/390, d1=0.581, d2=0.476 g=1.495\n",
            ">1, 286/390, d1=0.601, d2=0.472 g=1.660\n",
            ">1, 287/390, d1=0.624, d2=0.650 g=1.736\n",
            ">1, 288/390, d1=0.646, d2=0.498 g=1.589\n",
            ">1, 289/390, d1=0.652, d2=0.447 g=1.789\n",
            ">1, 290/390, d1=0.602, d2=0.450 g=1.705\n",
            ">1, 291/390, d1=0.617, d2=0.414 g=1.930\n",
            ">1, 292/390, d1=0.677, d2=0.434 g=1.705\n",
            ">1, 293/390, d1=0.594, d2=0.408 g=1.763\n",
            ">1, 294/390, d1=0.620, d2=0.406 g=1.864\n",
            ">1, 295/390, d1=0.527, d2=0.331 g=1.864\n",
            ">1, 296/390, d1=0.689, d2=0.453 g=1.959\n",
            ">1, 297/390, d1=0.626, d2=0.367 g=1.749\n",
            ">1, 298/390, d1=0.631, d2=0.430 g=1.752\n",
            ">1, 299/390, d1=0.522, d2=0.378 g=1.975\n",
            ">1, 300/390, d1=0.535, d2=0.364 g=1.987\n",
            ">1, 301/390, d1=0.553, d2=0.373 g=2.007\n",
            ">1, 302/390, d1=0.493, d2=0.274 g=2.233\n",
            ">1, 303/390, d1=0.672, d2=0.329 g=2.130\n",
            ">1, 304/390, d1=0.566, d2=0.333 g=2.211\n",
            ">1, 305/390, d1=0.487, d2=0.268 g=2.269\n",
            ">1, 306/390, d1=0.512, d2=0.241 g=2.317\n",
            ">1, 307/390, d1=0.569, d2=0.302 g=2.210\n",
            ">1, 308/390, d1=0.439, d2=0.370 g=2.255\n",
            ">1, 309/390, d1=0.452, d2=0.443 g=2.640\n",
            ">1, 310/390, d1=0.691, d2=0.423 g=2.429\n",
            ">1, 311/390, d1=0.708, d2=0.897 g=2.796\n",
            ">1, 312/390, d1=0.907, d2=0.271 g=2.629\n",
            ">1, 313/390, d1=1.085, d2=0.473 g=2.037\n",
            ">1, 314/390, d1=0.850, d2=0.436 g=1.992\n",
            ">1, 315/390, d1=0.701, d2=0.446 g=2.076\n",
            ">1, 316/390, d1=0.603, d2=0.431 g=2.208\n",
            ">1, 317/390, d1=0.670, d2=0.326 g=1.910\n",
            ">1, 318/390, d1=0.584, d2=0.436 g=1.923\n",
            ">1, 319/390, d1=0.579, d2=0.366 g=1.920\n",
            ">1, 320/390, d1=0.640, d2=0.406 g=2.094\n",
            ">1, 321/390, d1=0.734, d2=0.377 g=1.830\n",
            ">1, 322/390, d1=0.695, d2=0.429 g=1.864\n",
            ">1, 323/390, d1=0.616, d2=0.389 g=1.766\n",
            ">1, 324/390, d1=0.615, d2=0.407 g=2.048\n",
            ">1, 325/390, d1=0.518, d2=0.371 g=2.006\n",
            ">1, 326/390, d1=0.618, d2=0.320 g=1.882\n",
            ">1, 327/390, d1=0.431, d2=0.406 g=1.751\n",
            ">1, 328/390, d1=0.555, d2=0.453 g=1.850\n",
            ">1, 329/390, d1=0.618, d2=0.534 g=1.835\n",
            ">1, 330/390, d1=0.612, d2=0.484 g=1.928\n",
            ">1, 331/390, d1=0.562, d2=0.448 g=1.927\n",
            ">1, 332/390, d1=0.443, d2=0.509 g=2.062\n",
            ">1, 333/390, d1=0.493, d2=0.408 g=2.285\n",
            ">1, 334/390, d1=0.587, d2=0.232 g=2.380\n",
            ">1, 335/390, d1=0.511, d2=0.257 g=2.229\n",
            ">1, 336/390, d1=0.311, d2=0.296 g=2.216\n",
            ">1, 337/390, d1=0.251, d2=0.303 g=2.258\n",
            ">1, 338/390, d1=0.464, d2=0.349 g=1.902\n",
            ">1, 339/390, d1=0.440, d2=0.546 g=1.668\n",
            ">1, 340/390, d1=0.494, d2=0.507 g=1.518\n",
            ">1, 341/390, d1=0.439, d2=0.509 g=1.417\n",
            ">1, 342/390, d1=0.418, d2=0.550 g=1.330\n",
            ">1, 343/390, d1=0.486, d2=0.522 g=1.282\n",
            ">1, 344/390, d1=0.458, d2=0.463 g=1.243\n",
            ">1, 345/390, d1=0.323, d2=0.568 g=1.246\n",
            ">1, 346/390, d1=0.401, d2=0.511 g=1.334\n",
            ">1, 347/390, d1=0.369, d2=0.496 g=1.318\n",
            ">1, 348/390, d1=0.370, d2=0.469 g=1.453\n",
            ">1, 349/390, d1=0.451, d2=0.398 g=1.480\n",
            ">1, 350/390, d1=0.304, d2=0.391 g=1.612\n",
            ">1, 351/390, d1=0.429, d2=0.404 g=1.545\n",
            ">1, 352/390, d1=0.425, d2=0.388 g=1.485\n",
            ">1, 353/390, d1=0.390, d2=0.398 g=1.442\n",
            ">1, 354/390, d1=0.392, d2=0.384 g=1.500\n",
            ">1, 355/390, d1=0.247, d2=0.361 g=1.655\n",
            ">1, 356/390, d1=0.305, d2=0.323 g=1.691\n",
            ">1, 357/390, d1=0.358, d2=0.386 g=1.512\n",
            ">1, 358/390, d1=0.310, d2=0.442 g=1.649\n",
            ">1, 359/390, d1=0.366, d2=0.406 g=1.599\n",
            ">1, 360/390, d1=0.354, d2=0.434 g=1.753\n",
            ">1, 361/390, d1=0.413, d2=0.407 g=1.729\n",
            ">1, 362/390, d1=0.457, d2=0.442 g=1.772\n",
            ">1, 363/390, d1=0.395, d2=0.371 g=1.864\n",
            ">1, 364/390, d1=0.612, d2=0.440 g=1.610\n",
            ">1, 365/390, d1=0.465, d2=0.516 g=1.771\n",
            ">1, 366/390, d1=0.656, d2=0.410 g=1.885\n",
            ">1, 367/390, d1=0.636, d2=0.538 g=1.811\n",
            ">1, 368/390, d1=0.814, d2=0.441 g=1.754\n",
            ">1, 369/390, d1=0.621, d2=0.408 g=1.785\n",
            ">1, 370/390, d1=0.658, d2=0.445 g=1.806\n",
            ">1, 371/390, d1=0.647, d2=0.417 g=1.837\n",
            ">1, 372/390, d1=0.771, d2=0.486 g=1.865\n",
            ">1, 373/390, d1=0.591, d2=0.409 g=1.945\n",
            ">1, 374/390, d1=0.507, d2=0.358 g=1.852\n",
            ">1, 375/390, d1=0.645, d2=0.453 g=1.647\n",
            ">1, 376/390, d1=0.732, d2=0.697 g=1.545\n",
            ">1, 377/390, d1=0.569, d2=0.693 g=1.496\n",
            ">1, 378/390, d1=0.697, d2=0.696 g=1.256\n",
            ">1, 379/390, d1=0.615, d2=0.538 g=1.515\n",
            ">1, 380/390, d1=0.563, d2=0.364 g=1.834\n",
            ">1, 381/390, d1=0.520, d2=0.397 g=1.772\n",
            ">1, 382/390, d1=0.405, d2=0.389 g=1.837\n",
            ">1, 383/390, d1=0.448, d2=0.391 g=1.775\n",
            ">1, 384/390, d1=0.578, d2=0.456 g=1.535\n",
            ">1, 385/390, d1=0.377, d2=0.458 g=1.851\n",
            ">1, 386/390, d1=0.453, d2=0.382 g=1.939\n",
            ">1, 387/390, d1=0.410, d2=0.347 g=1.901\n",
            ">1, 388/390, d1=0.465, d2=0.418 g=1.839\n",
            ">1, 389/390, d1=0.397, d2=0.471 g=2.067\n",
            ">1, 390/390, d1=0.451, d2=0.433 g=2.198\n",
            ">2, 1/390, d1=0.429, d2=0.368 g=2.009\n",
            ">2, 2/390, d1=0.480, d2=0.748 g=1.861\n",
            ">2, 3/390, d1=0.459, d2=0.781 g=1.802\n",
            ">2, 4/390, d1=0.492, d2=0.590 g=1.761\n",
            ">2, 5/390, d1=0.379, d2=0.408 g=1.803\n",
            ">2, 6/390, d1=0.433, d2=0.453 g=2.006\n",
            ">2, 7/390, d1=0.478, d2=0.279 g=2.158\n",
            ">2, 8/390, d1=0.390, d2=0.276 g=1.939\n",
            ">2, 9/390, d1=0.268, d2=0.554 g=2.138\n",
            ">2, 10/390, d1=0.520, d2=0.550 g=2.362\n",
            ">2, 11/390, d1=0.658, d2=0.245 g=1.850\n",
            ">2, 12/390, d1=0.582, d2=0.481 g=1.510\n",
            ">2, 13/390, d1=0.286, d2=0.484 g=1.594\n",
            ">2, 14/390, d1=0.374, d2=0.583 g=1.633\n",
            ">2, 15/390, d1=0.670, d2=0.605 g=1.414\n",
            ">2, 16/390, d1=0.515, d2=0.721 g=1.280\n",
            ">2, 17/390, d1=0.486, d2=0.789 g=1.324\n",
            ">2, 18/390, d1=0.442, d2=0.493 g=1.464\n",
            ">2, 19/390, d1=0.750, d2=0.576 g=1.385\n",
            ">2, 20/390, d1=0.445, d2=0.469 g=1.453\n",
            ">2, 21/390, d1=0.499, d2=0.521 g=1.447\n",
            ">2, 22/390, d1=0.422, d2=0.443 g=1.520\n",
            ">2, 23/390, d1=0.445, d2=0.477 g=1.476\n",
            ">2, 24/390, d1=0.380, d2=0.600 g=1.393\n",
            ">2, 25/390, d1=0.356, d2=0.583 g=1.411\n",
            ">2, 26/390, d1=0.383, d2=0.503 g=1.426\n",
            ">2, 27/390, d1=0.386, d2=0.585 g=1.468\n",
            ">2, 28/390, d1=0.531, d2=0.488 g=1.741\n",
            ">2, 29/390, d1=0.359, d2=0.417 g=2.295\n",
            ">2, 30/390, d1=0.552, d2=0.327 g=2.484\n",
            ">2, 31/390, d1=0.592, d2=0.307 g=2.267\n",
            ">2, 32/390, d1=0.453, d2=0.340 g=2.513\n",
            ">2, 33/390, d1=0.589, d2=0.277 g=2.200\n",
            ">2, 34/390, d1=0.537, d2=0.429 g=2.090\n",
            ">2, 35/390, d1=0.463, d2=0.427 g=2.071\n",
            ">2, 36/390, d1=0.549, d2=0.364 g=2.298\n",
            ">2, 37/390, d1=0.505, d2=0.238 g=2.282\n",
            ">2, 38/390, d1=0.710, d2=0.498 g=1.775\n",
            ">2, 39/390, d1=0.454, d2=0.438 g=2.261\n",
            ">2, 40/390, d1=0.572, d2=0.312 g=2.220\n",
            ">2, 41/390, d1=0.576, d2=0.368 g=2.044\n",
            ">2, 42/390, d1=0.453, d2=0.410 g=1.877\n",
            ">2, 43/390, d1=0.634, d2=0.496 g=1.985\n",
            ">2, 44/390, d1=0.493, d2=0.416 g=2.066\n",
            ">2, 45/390, d1=0.442, d2=0.330 g=2.038\n",
            ">2, 46/390, d1=0.416, d2=0.359 g=2.213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGvn25WTA1oz",
        "outputId": "6ba3d1da-c563-4028-f0b6-940349344391"
      },
      "source": [
        "img1 = \"\"\n",
        "img2 = \"\"\n",
        "\n",
        "src1 = cv2.imread(img1)\n",
        "src2 = cv2.imread (img2)\n",
        "dst = cv2.addWeighted (src1, 0.5, src2, 0.5, 0)\n",
        "cv2.imshow ('output', dst)\n",
        "cv2.waitKey (0)\n",
        "cv2.destroyAllWindows ()\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 4096)              413696    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 8, 8, 128)         524416    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 3)         3459      \n",
            "=================================================================\n",
            "Total params: 1,466,115\n",
            "Trainable params: 1,466,115\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDL074IMA7Oq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}